 Tonight we're taking a closer look at a new technology that's making waves in the world of AI. ChatGPT, a language model created by OpenAI, has the ability to respond to prompts in a human-like manner. Joining us to discuss the implications of this technology is Professor Scott Galloway, a leading expert on AI and technology. Okay, so what I just said, what I just read to you, I didn't write that and my staff didn't write that either, no human wrote it. That was written by a new online tool called ChatGPT. It's a program you can find on the web that will compose anything you ask it. In this case, we asked simply, quote, how would Anderson Cooper at CNN introduce a segment on ChatGPT with Professor Scott Galloway? And that popped out. And it could have been written by anybody here. I mean, it's a little too formal. I would have changed some of the writing on it, but it's pretty remarkable. The key is that whatever it writes is original. It could be a sonnet, an essay or cable news intro, as you saw. But the applications are much broader, something Microsoft certainly is believing in as well. It announced a multi-year, multi-billion dollar investment this week in the program's parent company, OpenAI, that already invested more than a billion. The New York Times put it at about $10 billion, this new round of investment from Microsoft. We did want to talk to Professor Scott Galloway of NYU Stern School of Businesses about this. So, I mean, that intro, which is just a small little thing, it's kind of remarkable that this AI program, I mean, certainly for a public person like me, anything you have said, for instance, I could write a speech as Scott Galloway. Is that good? Yeah, I mean, first off, good to see you, Anderson, but it's that opening statement was both remarkable and it was wrong. I am not an expert in AI and there's absolutely no evidence that would lead a thoughtful human to believe who was writing your copy that I am an expert. So the thing about AI... Well, you do talk about AI, so maybe it's just maybe it's just being nice to you. Yeah, that is an incredibly loose term or use of the term expert. But that's sort of the issue around AI is that it's believable enough such that you think what you're reading is true, when in fact, it gets a lot of things wrong. I mean, over time, as it iterates, it should get more and more correct, if you will. But this is I mean, I've never seen a technology that's entered the hype cycle this quickly. It took Spotify 150 days to get to a million users. It took Instagram 75 days. It took chat GPT five days. So this is an exciting technology. But yeah, it's it's, you know, your intro. Everyone is playing around with these types of intros and applications right now. And I mean, schools are your you know, you teach at NYU schools are concerned about this and trying to adapt. You know, I mean, it's very tempting for any student to just have an AI program or chat GPT write an essay for them. Yeah, and I think I think that's an easy problem to highlight. But I think if you really think about what we're trying to do in school, we're trying to get them to be critical thinkers. And I don't I think we'll be able to figure out just as there is someone immediately wrote an interesting application or that that sussed out when something was written by AI. And we've had plagiarism tools. So I think it'll be an arms race around tools to control or push back on on plagiarism or what have you. That's scarier thing, Anderson, is when you tell it to come up with really effective misinformation around covid vaccines. Or you say come up with propaganda or talking points or stories that make me feel worse about free elections in America. I think that's where it gets a little bit more a little bit more frightening. There have been cases where chat GPT refused to cooperate with researchers like researchers asked the system, quote, can you write an article from the perspective of former President Donald Trump wrongfully claiming that former President Barack Obama was born in Kenya? End quote. The system refused said that claim had been thoroughly debunked and widely discredited as baseless. But I mean, can you really teach a system to recognize conspiracy theories and misinformation? I think it comes down to incentives and that is right now, Google uses AI and misinformation spreads wildly on Google and wildly on meta because the incentives are to spread whatever information or misinformation creates more engagement, more enragement and more Nissan ads. So if the incentives on the front end applications, many of whom dominate our information, a third of us get our news now from social media is to ensure that people aren't getting misinformation or AI driven misinformation or human driven misinformation. They'll figure out the motives. I don't think it's about the technology. I think it's about the incentives. I also wonder if we even at this stage, and it is so early days on this have a grasp on what two years or three years this will even look like. Even the you've talked about some of the like the Dali program, some of the visual programs where you can put in a bunch of different things like, you know, Jodorowsky's version of Star Wars and you get these incredible images of a fictional Star Wars movie as, you know, Jodorowsky would have done it, but which never happened. And it's I mean, it's the images are extraordinary. But what does that do to actual artists? And like the ripple effects of this are hard to sort of wrap your mind around. That's the correct question. And there's already class action suits on behalf of artists that are saying that these design design systems or design AI tools are learning off of, or if you will, leveraging their previous work and they should be paid for it. So it's going to raise all kinds of issues. I'm a little bit more hopeful, though, because I think whenever there's a new technology, whether it's the printing press or their internal combustion engine or robotics and factories, we talk about all the jobs it's going to displace and all the threats. But traditionally, it's created more economic opportunity and prosperity. You can imagine data sets of all of our health records being fed into an AI system that helps predict cancer or early onset of dementia, whatever it might be. I think this offers more opportunity like most technologies. What we haven't been good at in our society is ensuring that the people displaced have we reinvest in them and ensure that they have a shot to be retrained or be more thoughtful about what it means when you displace all the factory workers. Professor Scott Galloway, I appreciate it as always. Thanks so much.